install.packages('Sleuth2')
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
data(case2002)
library(Sleuth2)
data(case2002)
View(case2002)
plot(case2002)
?logit
fit <- glm(LC ~ ., data=case2002)
apply(case2002, 2, class)
case2002[, 5:7] <- as.numeric(case2002[, 5:7])
case2002[, 5:7] <- sapply(case2002[, 5:7], as.numeric)
fit <- glm(LC ~ ., data=case2002)
fit <- glm(LC ~ ., data=case2002)
fit <- glm(LC ~ ., family = "binomial",, data=case2002)
summary(fit)
fit2 <- glm(LC ~ -BK,family = 'binomial', data=case2002)
summary(fit)
fit2 <- glm(LC ~ . -BK,family = 'binomial', data=case2002)
summary(fit)
fit2 <- glm(LC ~ . -BK,family = 'binomial', data=case2002)
summary(fit2)
fit3 <- glm(LC ~ BK + YR, family = 'binomial', data=case2002)
summary(fit3)
anova(fit3, fit, test='Chisq')
1 - fit3$deviance/fit3$null.deviance
aic(fit3, fit)
AIC(fit3, fit)
?AIC
BIC(fit3, fit)
1- fit3$deviance/fit3$null.deviance
1- fit$deviance/fit$null.deviance
summary(fit3)
mean(case2002$YR)
years_smoking <- mean(case2002$YR)
?repdict
?predict
predict(fit3, c(YR=years_smoking, BK='Bird') type = 'response')
predict(fit3, c(YR=years_smoking, BK='Bird'), type = 'response')
predict(fit3, data.frame(YR=years_smoking, BK='Bird'), type = 'response')
predict(fit3, data.frame(YR=years_smoking, BK='NoBird'), type = 'response')
predict(fit3, data.frame(YR=0, BK='Bird'), type='response')
predict(fit3, data.frame(YR=0, BK='NoBird'), type='response')
predict(fit3, case2002, type='response')
fit3$fitted.values
years_smoking <- mean(case2002$YR)
log_odds <- predict(fit3, data.frame(YR=years_smoking, BK='Bird'), type = 'response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
prob
log_odds <- predict(fit3, data.frame(YR=years_smoking, BK='NoBird'), type = 'response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
prob
log_odds <- predict(fit3, data.frame(YR=0, BK='Bird'), type='response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
prob
log_odds <- predict(fit3, data.frame(YR=0, BK='NoBird'), type='response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
prob
log_odds <- predict(fit3, case2002, type='response')
odds <- exp(log_odds)
prob <- odd(1+odds)
log_odds <- predict(fit3, case2002, type='response')
odds <- exp(log_odds)
prob <- odd/(1+odds)
log_odds <- predict(fit3, case2002, type='response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
case2002$predict <- ifelse(prob>0.5, 'LungCancer', 'NoCancer')
min(prob)
#If you implement correctly, the code below:
# install.packages('optimx')
library(optimx)
install.packages('optimx')
model = glm(LC ~ YR, family = "binomial", data = case2002)
summary(model)
?optimx
#If you implement correctly, the code below:
# install.packages('optimx')
library(optimx)
?optimx
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
set.seed(0)
x = model.matrix(lpsa ~ ., prostate)[,-1]
y = prostate$lpsa
train = sample(1:nrow(x), .8*nrow(x))
test = -train
y.test = y[test]
library(glmnet)
install.packages('glmnet')
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
set.seed(0)
x = model.matrix(lpsa ~ ., prostate)[,-1]
y = prostate$lpsa
train = sample(1:nrow(x), .8*nrow(x))
test = -train
y.test = y[test]
library(glmnet)
grid = 10^seq(5, -2, length=100)
ridge.fit = glmnet(x[train,], y[train], alpha = 0, lambda=grid)
coefs = ridge.fit$beta
plot(coefs, xvar='lambda', label = TRUE, main='Ridge Regression')
set.seed(0)
fit.cv <- cv.glmnet(x[train,], y[train],lambda = grid,nfolds = 10, alpha=0)
plot(fit.cv)
fit.cv$lambda.min
fit.test <- predict(fit.cv, s = fit.cv$lambda.min, newx = x[test,])
mean((fit.test - y.test)^2)
#Lambda is the degree to which you are adding a correction the term. If it is 0, you are getting the same as linear regression
#If it is too large, it will overwhelm your betas sending them all to 0.
#It gives the opportunity to use all the data for training and validation. It allows you to reduce the effects of randomly choosing simply
#one train and one validation. By taking the mean of the values, you have more robust estimates.
#The general rule is 5 or 10 folds. More folds mean more data is recycled, however, you are validating on a smaller portion of your data
#which can introduce it's own problems, especially if the data set is small.
fit.lasso = glmnet(x[train,], y[train], alpha=1, lambda = grid)
coefs = fit.lasso$beta
plot(coefs, xvar='lambda', label = TRUE, main='Lasso Regression')
set.seed(0)
fit.lasso.cv = cv.glmnet(x = x[train,],y = y[train],lambda = grid, nfolds = 10, alpha=1)
plot(fit.lasso.cv)
fit.lasso.cv$lambda.min
fit.lasso.test <- predict(fit.lasso.cv, s=fit.lasso.cv$lambda.min, newx = x[test,])
mean((fit.lasso.test - y[test])^2)
#Ridge had a slightly smaller MSE
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
prostate = read.table("https://s3.amazonaws.com/nycdsabt01/Prostate.txt", header = TRUE)
x = model.matrix(lpsa ~ ., prostate)[, -1]
y = prostate$lpsa
set.seed(0)
train = sample(1:nrow(x), nrow(x)*0.8)
test = (-train)
y.test = y[test]
length(train)/nrow(x)  # 0.7938144
length(y.test)/nrow(x) # 0.2061856
grid = 10^seq(5, -2, length = 100)
#  Fit the ridge regression. Alpha = 0 for ridge regression.
library(glmnet)
ridge.models = glmnet(x[train, ], y[train], alpha = 0, lambda = grid)
plot(ridge.models, xvar = "lambda", label = TRUE, main = "Ridge Regression")
#  Comment on the shrinkage
#The coefficients all seem to shrink towards 0 as lambda gets quite large.
#Most coefficients are very close to 0 once the log lambda value gets to about 5.
#However, in ridge regression coefficients are never exactly 0.
plot(ridge.fit, xvar='lambda', label = TRUE, main='Ridge Regression')
set.seed(0)
cv.ridge.out = cv.glmnet(x[train, ], y[train], alpha = 0, nfolds = 10, lambda = grid)
plot(cv.ridge.out, main = "Ridge Regression\n")
#The error seems to be reduced with a log lambda value of around -2.0002;
#this corresponts to a lambda value of about 0.135. This is the value of lambda
#we should move forward with in our future analyses.
fit.lasso = glmnet(x[train,], y[train], alpha=1, lambda = grid)
coefs = fit.lasso$beta
plot(fit.lasso, xvar='lambda', label = TRUE, main='Lasso Regression')
fit.lasso = glmnet(x[train,], y[train], alpha=1, lambda = grid)
coefs = fit.lasso$beta
plot(fit.lasso, xvar='lambda', label = TRUE, main='Lasso Regression')
set.seed(0)
fit.lasso.cv = cv.glmnet(x = x[train,],y = y[train],lambda = grid, nfolds = 10, alpha=1)
plot(fit.lasso.cv)
fit.lasso.cv$lambda.min
refit.lasso <- cv.glmnet(x = x[train,],y = y[train],lambda = fit.lasso.cv$lambda.min, nfolds = 10, alpha=1)
refit.lasso <- glmnet(x[train,], y[train], alpha=1, lambda = fit.lasso.cv$lambda.min)
fit.lasso.test <- predict(refit.lasso, s=fit.lasso.cv$lambda.min, newx = x[test,])
mean((fit.lasso.test - y[test])^2)
set.seed(0)
fit.lasso.cv = cv.glmnet(x = x[train,],y = y[train],lambda = grid, nfolds = 10, alpha=1)
plot(fit.lasso.cv)
fit.lasso.cv$lambda.min
refit.lasso <- glmnet(x[train,], y[train], alpha=1, lambda = fit.lasso.cv$lambda.min)
fit.lasso.test <- predict(refit.lasso, s=fit.lasso.cv$lambda.min, newx = x[test,])
mean((fit.lasso.test - y[test])^2)
mean((fit.lasso.test - y[test])^2)
install.packages('Sleuth2')
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(Sleuth2)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
plot(case2002[, 5:7], col = case2002$LC)
plot(case2002)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(Sleuth2)
data(case2002)
plot(case2002)
#AG, YR, CD seem to have relationships with each other
case2002[, 5:7] <- sapply(case2002[, 5:7], as.numeric)
fit <- glm(LC ~ ., family = "binomial",, data=case2002)
summary(fit)
#Most factors are not significant
#The log odds of a woman getting lung cancer is 0.56 greater than that of a man's
fit2 <- glm(LC ~ . -BK,family = 'binomial', data=case2002)
#This model is worse as the AIC is higher
#Residual devianceis higher, 166 vs 154, suggesting a better fit.
fit3 <- glm(LC ~ BK + YR, family = 'binomial', data=case2002)
anova(fit3, fit, test='Chisq')
#p value is 0.4, so we accept the null hypothese that the model with more factors does not add value
AIC(fit3, fit)
#      df      AIC
# fit3  3 164.1144
# fit   7 168.1984
BIC(fit3, fit)
#      df      BIC
# fit3  3 173.0857
# fit   7 189.1314
#R^2_dev
1- fit3$deviance/fit3$null.deviance
#0.1550791
1- fit$deviance/fit$null.deviance
#0.1760051
#BIC and AIC are lower for fit3, plus it uses fewer predictors (though this is accounted for in AIC/BIC). Furthermore,
#all the predictores in fit3 are significant whereas they were not in the original model
years_smoking <- mean(case2002$YR)
log_odds <- predict(fit3, data.frame(YR=years_smoking, BK='Bird'), type = 'response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
prob
#.62
log_odds <- predict(fit3, data.frame(YR=years_smoking, BK='NoBird'), type = 'response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
prob
#.54
log_odds <- predict(fit3, data.frame(YR=0, BK='Bird'), type='response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
prob
#.54
log_odds <- predict(fit3, data.frame(YR=0, BK='NoBird'), type='response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
prob
#0.51
log_odds <- predict(fit3, case2002, type='response')
odds <- exp(log_odds)
prob <- odds/(1+odds)
case2002$predict <- ifelse(prob>0.5, 'LungCancer', 'NoCancer')
#Very badly, it predicts everyone has lung cancer, the lowest probability is 0.51
neg_log_likelihood<-function(intercept, slope){
# Your code here
}
#If you implement correctly, the code below:
# install.packages('optimx')
library(optimx)
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(Sleuth2)
summary(case2002)
sapply(case2002[, 5:7], sd)
# Why do we calculate sd here?
# We want to find those variables that have large variances
plot(case2002[, 5:7], col = case2002$LC)
#The indicator for having lung cancer seems to split the data at more extreme
#values of our continuous variables; however, we might be concerned with some
#multicollinearity with the year variable in particular and should check this
#later on.
#--------------------- Another scatterplot matrix option ---------------------
library(car)
scatterplotMatrix(~AG+YR+CD|LC,data=case2002[, c(1,5:7)])
logit.overall = glm(LC ~ ., family = "binomial", data = case2002)
# H0: The logistic regression model is appropriate.
# H1: The logistic regression model is not appropriate.
pchisq(logit.overall$deviance, logit.overall$df.residual, lower.tail = FALSE)
logit.overall = glm(LC ~ ., family = "binomial", data = case2002)
logit.overall = glm(LC ~ ., family = "binomial", data = case2002)
library(Sleuth2)
summary(case2002)
sapply(case2002[, 5:7], sd)
# Why do we calculate sd here?
# We want to find those variables that have large variances
#--------------------- Another scatterplot matrix option ---------------------
library(car)
logit.overall = glm(LC ~ ., family = "binomial", data = case2002)
# H0: The logistic regression model is appropriate.
# H1: The logistic regression model is not appropriate.
pchisq(logit.overall$deviance, logit.overall$df.residual, lower.tail = FALSE)
# Why do we use lower.tail here?
# http://stats.stackexchange.com/questions/22347/is-chi-squared-always-a-one-sided-test
#The p-value for the overall goodness of fit test is about 0.20,
#which is greater than the cutoff value of 0.05. We do not have
#evidence to reject the null hypothesis that the model is appropriate.
logit.overall$coefficients
# ---- OR -----
summary(logit.overall)
#The log odds of having lung cancer are increased by about 0.56 if you
#are a female as compared to a male, holding all other variables constant.
logit.noBK = glm(LC ~ . - BK, family = "binomial", data = case2002)
#overall goodness of fit test
pchisq(logit.noBK$deviance, logit.noBK$df.residual, lower.tail = FALSE)
#The p-value for the overall goodness of fit test is about 0.07, which is greater
#than the cutoff value of 0.05. We do not have evidence to reject the null
#hypothesis that the model is appropriate.
pchisq(logit.noBK$deviance - logit.overall$deviance,
logit.noBK$df.residual - logit.overall$df.residual,
lower.tail = FALSE)
#------------------------------------------------Note-----------------------------------------
# Likelihood ratio tests are similar to partial F-tests in the sense that they compare
# the full model with a restricted model where the explanatory variables of interest are omitted.
# The p-values of the tests are calculated using the Chi-square distribution.
#----------------------------------------------------------------------------------------------
# More simply,a likelihood ratio test comparing the full and reduced models can be performed
# using the anova() function with the additional option test="Chisq".
anova(logit.noBK, logit.overall, test = "Chisq")
#The p-value for the drop in deviance test is < 0.001, which is quite significant.
#We reject the null hypothesis that the coefficient for the birdkeeping variable
#is 0, and conclude that having it in the model should provide a better fit.
#i.e., the full model is better than the reduced model
newdata = with(case2002, data.frame(YR = mean(YR),
BK = factor(c("Bird", "NoBird"))))
cbind(newdata, "Prob. Lung Cancer" = predict(logit.justBKYR, newdata, type = "response"))
newdata = data.frame(YR = 0, BK = factor(c("Bird", "NoBird")))
cbind(newdata, "Prob. Lung Cancer" = predict(logit.justBKYR, newdata, type = "response"))
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message=FALSE)
library(Sleuth2)
summary(case2002)
sapply(case2002[, 5:7], sd)
# Why do we calculate sd here?
# We want to find those variables that have large variances
plot(case2002[, 5:7], col = case2002$LC)
#The indicator for having lung cancer seems to split the data at more extreme
#values of our continuous variables; however, we might be concerned with some
#multicollinearity with the year variable in particular and should check this
#later on.
#--------------------- Another scatterplot matrix option ---------------------
library(car)
scatterplotMatrix(~AG+YR+CD|LC,data=case2002[, c(1,5:7)])
logit.overall = glm(LC ~ ., family = "binomial", data = case2002)
# Note that the option family is set to binomial,
# which tells R to perform logistic regression.
# H0: The logistic regression model is appropriate.
# H1: The logistic regression model is not appropriate.
pchisq(logit.overall$deviance, logit.overall$df.residual, lower.tail = FALSE)
# Why do we use lower.tail here?
# http://stats.stackexchange.com/questions/22347/is-chi-squared-always-a-one-sided-test
#The p-value for the overall goodness of fit test is about 0.20,
#which is greater than the cutoff value of 0.05. We do not have
#evidence to reject the null hypothesis that the model is appropriate.
logit.overall$coefficients
# ---- OR -----
summary(logit.overall)
#The log odds of having lung cancer are increased by about 0.56 if you
#are a female as compared to a male, holding all other variables constant.
logit.noBK = glm(LC ~ . - BK, family = "binomial", data = case2002)
#overall goodness of fit test
pchisq(logit.noBK$deviance, logit.noBK$df.residual, lower.tail = FALSE)
#The p-value for the overall goodness of fit test is about 0.07, which is greater
#than the cutoff value of 0.05. We do not have evidence to reject the null
#hypothesis that the model is appropriate.
pchisq(logit.noBK$deviance - logit.overall$deviance,
logit.noBK$df.residual - logit.overall$df.residual,
lower.tail = FALSE)
#------------------------------------------------Note-----------------------------------------
# Likelihood ratio tests are similar to partial F-tests in the sense that they compare
# the full model with a restricted model where the explanatory variables of interest are omitted.
# The p-values of the tests are calculated using the Chi-square distribution.
#----------------------------------------------------------------------------------------------
# More simply,a likelihood ratio test comparing the full and reduced models can be performed
# using the anova() function with the additional option test="Chisq".
anova(logit.noBK, logit.overall, test = "Chisq")
#The p-value for the drop in deviance test is < 0.001, which is quite significant.
#We reject the null hypothesis that the coefficient for the birdkeeping variable
#is 0, and conclude that having it in the model should provide a better fit.
#i.e., the full model is better than the reduced model
logit.justBKYR = glm(LC ~ BK + YR, family = "binomial", data = case2002)
pchisq(logit.justBKYR$deviance - logit.overall$deviance,
logit.justBKYR$df.residual - logit.overall$df.residual,
lower.tail = FALSE)
#---------OR---------------
anova(logit.justBKYR, logit.overall, test = "Chisq")
#The p-value for the drop in deviance test is quite large at 0.4175, indicating
#that the null hypothesis that the coefficients of gender, socioeconomic status,
#age, and average rate of smoking are jointly not adding any predictive power
#to our analysis. The reduced model is sufficient.
AIC(logit.overall, logit.noBK, logit.justBKYR)
BIC(logit.overall, logit.noBK, logit.justBKYR)
1 - logit.overall$deviance/logit.overall$null.deviance
1 - logit.noBK$deviance/logit.noBK$null.deviance
1 - logit.justBKYR$deviance/logit.justBKYR$null.deviance
#The AIC and BIC are minimized for the most simplified model, indicating that it
#is most preferable. While the McFadden's R^2 term isn't maximized for the most
#simplified model (only about 15.5% as compared to about 17.6% for the overall
#model with all coefficients), we choose to move forward with the simplified
#model because it has relatively high predictive power alongside simplicity.
#---------------------How to interpret the McFadden's R^2------------------------
#http://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation
newdata = with(case2002, data.frame(YR = mean(YR),
BK = factor(c("Bird", "NoBird"))))
cbind(newdata, "Prob. Lung Cancer" = predict(logit.justBKYR, newdata, type = "response"))
newdata = data.frame(YR = 0, BK = factor(c("Bird", "NoBird")))
cbind(newdata, "Prob. Lung Cancer" = predict(logit.justBKYR, newdata, type = "response"))
#------------------------------------------Note---------------------------------------
# The usage is similar to that of the function predict which we previously used when working
# on multiple linear regression problems. The main difference is the option type, which tells
# R which type of prediction is required.
#-------------------------------------------------------------------------------------------
# The default predictions are given on the logit scale (i.e.predictions are made in terms of the log odds),
# while using type = "response"
LC.predicted = round(logit.justBKYR$fitted.values)
table(truth = case2002$LC, prediction = LC.predicted)
(85+22)/147
98/147
#The model performs decently well with an approximate 72.79% accuracy; this is
#a bit above the baseline of 66.67% accuracy if we were to simply guess that
#every individual has no lung cancer within our dataset.
# Why do we choose the larger group 98/147 as our baseline?
# Because if we don't do any analysis on this and a new sample comes in,
# we will assign the most likely category to the new one. So this is the base line.
X = case2002$YR
is_LC = as.numeric(case2002$LC == 'LungCancer')
neg_log_likelihood<-function(intercept, slope){
t = slope*X+intercept
prob = 1/(1+exp(-t))
e = 1e-8
tmp = sapply(is_LC*prob + (1-is_LC)*(1-prob), function(i) min(max(i, e), 1-e))
return(-1*sum(log(tmp)))
}
#If you implement correctly, the code below:
# install.packages('optimx')
library(optimx)
years_smoking <- mean(case2002$YR)
log_odds <- predict(fit3, data.frame(YR=years_smoking, BK='Bird'), type = 'response')
LC.predicted = round(logit.justBKYR$fitted.values)
table(truth = case2002$LC, prediction = LC.predicted)
setwd('/Users/michaelsankari/Documents/NYC Data Science/Machine Learning Project/github/ML-project-for-NYCDSA')
df <- read.csv('./house-prices-advanced-regression-techniques/cleandata/s2_clean_dummified.csv')
View(df)
colnames(df)
fit <- lm(LogSalePrice~., data = df)
summary(fit)
vif(fit)
library(car)
alias(lm(LogSalePrice~., data = df))
df <- df[, -c('Exterior1st_ImStucc', 'Exterior1st_Stone', 'HouseStyle_2.5Fin', 'Id')]
drops <- c('Exterior1st_ImStucc', 'Exterior1st_Stone', 'HouseStyle_2.5Fin', 'Id')
df <- df[, !(names(df) %in% drops)]
df <- read.csv('./house-prices-advanced-regression-techniques/cleandata/s2_clean_dummified.csv')
drops <- c('Exterior1st_ImStucc', 'Exterior1st_Stone', 'HouseStyle_2.5Fin', 'Id')
df <- df[, !(names(df) %in% drops)]
fit <- lm(LogSalePrice~., data = df)
summary(fit)
vif(fit)
alias(lm(LogSalePrice~., data = df))
?alias
df$BsmtCond_None <- NULL
fit <- lm(LogSalePrice~., data = df)
vif(fit)
vif_fit <- vif(fit)
class(vif_fit)
t(vif_fit)
data.frame(vif_fit)
vif_fit <- data.frame(vif(fit))
View(vif_fit)
vif_fit$variable <- row.names(vif_fit)
vif_fit <- data.frame(variable = row.names(vif_fit), vif_score=vif.fit.)
vif_fit <- data.frame(variable = row.names(vif_fit), vif_score=vif.fit)
colnames(vif_fit)
vif_fit <- data.frame(variable = row.names(vif_fit), vif_score=vif_fit$vif.fit.)
vif_fit <- vif_fit(order(vif_fit$vif_score, decreasing = TRUE))
vif_fit <- vif_fit[order(vif_fit$vif_score, decreasing = TRUE),]
row.names(vif_fit) <- 1:nrow(vif_fit)
summary(fit)
vif_fit$variable[vif_fit<5]
vif_fit$variable[vif_fit$vif_score<5]
class(vif_fit$variable)
vif_fit$variable <- as.character(vif_fit$variable)
vif_fit$variable[vif_fit$vif_score<5]
keep <- vif_fit$variable[vif_fit$vif_score<5]
df <- df[, (names(df) %in% keep)]
fit2 <- lm(LogSalePrice~., data = df)
keep <- c(keep, 'LogSalePrice')
df <- df[, (names(df) %in% keep)]
df <- read.csv('./house-prices-advanced-regression-techniques/cleandata/s2_clean_dummified.csv')
df <- df[, !(names(df) %in% drops)]
df <- df[, (names(df) %in% keep)]
fit2 <- lm(LogSalePrice~., data = df)
summary(fit)
fit2 <- lm(LogSalePrice~., data = df)
summary(fit2)
vif(fit2)
#Which ones are less than 5
vif_fit$variable[vif_fit$vif_score<3]
keep <- vif_fit$variable[vif_fit$vif_score<5]
keep <- c(keep, 'LogSalePrice')
df <- df[, (names(df) %in% keep)]
fit2 <- lm(LogSalePrice~., data = df)
summary(fit2)
#Which ones are less than ...
keep <- vif_fit$variable[vif_fit$vif_score<3]
keep <- c(keep, 'LogSalePrice')
df <- df[, (names(df) %in% keep)]
fit2 <- lm(LogSalePrice~., data = df)
summary(fit2)
AIC(fit, fit2)
anova(fit2, fit)
write.csv(vif_fit, file = 'vif_scores.csv')
write.csv(vif_fit, file = 'vif_scores.csv', row.names = FALSE)
